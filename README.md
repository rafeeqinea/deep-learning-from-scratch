# ğŸ§© Deep Learning Book (2016) â€” Chapter-Scoped, NumPy-Only Lab

A faithful, hands-on rebuild of core ideas from **Goodfellow, Bengio, Courville (2016)** â€” but engineered as **chapter mini-packages**.  
Each chapter is a clean, importable API; the final **MNIST MLP app** composes *only* from these APIs.

> **Rule #1:** NumPy-only. No torch / tf / jax / autograd frameworks.  
> **Rule #2:** Small, testable modules. Chapter â†’ section â†’ tiny helpers.  
> **Rule #3:** The MNIST app does not bypass chapter APIs.

---

## ğŸ—ºï¸ Repository Layout (Top Level)

```
dl-notes-lab/
â”œâ”€ chapters/              # chapter-scoped, reusable mini-packages
â”‚  â”œâ”€ ch02_linear_algebra/
â”‚  â”œâ”€ ch03_probability/
â”‚  â”œâ”€ ch04_numerical/
â”‚  â”œâ”€ ch05_ml_basics/
â”‚  â”œâ”€ ch06_feedforward/
â”‚  â”œâ”€ ch07_regularization/
â”‚  â”œâ”€ ch08_optimization/
â”‚  â””â”€ __init__.py
â”œâ”€ app_mnist/            # final MNIST MLP that uses chapter APIs
â”‚  â”œâ”€ data_mnist.py
â”‚  â”œâ”€ model_mlp.py
â”‚  â”œâ”€ train.py
â”‚  â”œâ”€ metrics.py
â”‚  â””â”€ tests/
â”œâ”€ main.py               # CLI router (run chapter demos / train app)
â””â”€ README.md
```

---

## ğŸ§± Chapter Packages (Full Blueprint)

Each chapter folder contains **section folders** (`sX_Y/`).  
Each section has a **section main** (`sX_Y.py`) that re-exports tiny helpers so imports stay tidy.

### ch02_linear_algebra/  *(all utilities used later; no external libs)*

```
chapters/ch02_linear_algebra/
â”œâ”€ s2_1/                 # 2.1 Scalars, Vectors, Matrices and Tensors
â”‚  â”œâ”€ s2_1.py
â”‚  â”œâ”€ shapes.py          # infer_shape, ensure_vector/matrix/tensor_nd
â”‚  â”œâ”€ dtypes.py          # as_float32/64, is_integer_array
â”‚  â””â”€ basic_validators.py # same_shape, broadcastable, finite
â”œâ”€ s2_2/                 # 2.2 Multiplying Matrices and Vectors
â”‚  â”œâ”€ s2_2.py
â”‚  â”œâ”€ matvec.py          # mm, mv, vv (dot), safe_matmul
â”‚  â””â”€ linear_comb.py     # lincomb, affine_map
â”œâ”€ s2_3/                 # 2.3 Identity and Inverse
â”‚  â”œâ”€ s2_3.py
â”‚  â”œâ”€ identity.py        # eye, projector(U)=U U^T
â”‚  â””â”€ inverse_solve.py   # solve (preferred), is_invertible, maybe_inverse
â”œâ”€ s2_4/                 # 2.4 Linear Dependence and Span
â”‚  â”œâ”€ s2_4.py
â”‚  â”œâ”€ span_rank.py       # rank, is_full_rank, span_contains
â”‚  â””â”€ basis.py           # orthonormal_basis via SVD/QR
â”œâ”€ s2_5/                 # 2.5 Norms
â”‚  â”œâ”€ s2_5.py
â”‚  â””â”€ norms.py           # l1, l2, linf, Frobenius, op_norm_2 (Ïƒ_max)
â”œâ”€ s2_6/                 # 2.6 Special Matrices/Vectors
â”‚  â”œâ”€ s2_6.py
â”‚  â””â”€ special.py         # is_diagonal, is_symmetric, is_orthogonal, is_psd
â”œâ”€ s2_7/                 # 2.7 Eigendecomposition
â”‚  â”œâ”€ s2_7.py
â”‚  â””â”€ eigendecomp.py     # eig_sym, spectral_decomp, is_positive_definite
â”œâ”€ s2_8/                 # 2.8 SVD
â”‚  â”œâ”€ s2_8.py
â”‚  â””â”€ svd_tools.py       # svd_thin, cond_number, low_rank_approx
â”œâ”€ s2_9/                 # 2.9 Mooreâ€“Penrose Pseudoinverse
â”‚  â”œâ”€ s2_9.py
â”‚  â””â”€ pseudoinverse.py   # pinv_svd, min_norm_solve
â”œâ”€ s2_10/                # 2.10 Trace
â”‚  â”œâ”€ s2_10.py
â”‚  â””â”€ trace_ops.py       # trace, trace_cyclic_equal
â”œâ”€ s2_11/                # 2.11 Determinant
â”‚  â”œâ”€ s2_11.py
â”‚  â””â”€ determinant.py     # det, logdet_safe, volume_scale
â”œâ”€ s2_12/                # 2.12 PCA
â”‚  â”œâ”€ s2_12.py
â”‚  â””â”€ pca.py             # pca_svd, explained_variance, project_to_k
â”œâ”€ api.py                # chapter re-exports
â””â”€ tests/                # per-section tests
```

**Ch02 exports used later:** shape/dtype checks, safe matmul/affine, norms, SVD/cond, pinv, logdet, PCA helpers.

---

### ch03_probability/

```
chapters/ch03_probability/
â”œâ”€ s3_5/
â”‚  â”œâ”€ s3_5.py
â”‚  â””â”€ conditionals.py    # discrete/continuous slice-then-renormalize
â”œâ”€ s3_6/
â”‚  â”œâ”€ s3_6.py
â”‚  â””â”€ factorization.py   # chain-rule utilities (factorize joints)
â”œâ”€ s3_7/
â”‚  â”œâ”€ s3_7.py
â”‚  â””â”€ expectation.py     # empirical_expectation, variance, covariance
â”œâ”€ s3_8/
â”‚  â”œâ”€ s3_8.py
â”‚  â””â”€ information.py     # entropy, cross_entropy, KL (discrete)
â”œâ”€ s3_9/
â”‚  â”œâ”€ s3_9.py
â”‚  â””â”€ ml_map.py          # NLL from logits (one-hot), Î» â†” ÏƒÂ² (MAP prior)
â”œâ”€ s3_10/
â”‚  â”œâ”€ s3_10.py
â”‚  â””â”€ prob_numerics.py   # CE shift-invariance assertions, safety checks
â”œâ”€ api.py
â””â”€ tests/
```

**Ch03 exports used later:** CE/entropy/KL semantics for README plots and invariance tests (the MNIST app computes CE via Ch04/Ch06).

---

### ch04_numerical/

```
chapters/ch04_numerical/
â”œâ”€ s4_1/
â”‚  â”œâ”€ s4_1.py
â”‚  â””â”€ floating_point.py  # eps, overflow/underflow demos
â”œâ”€ s4_2/
â”‚  â”œâ”€ s4_2.py
â”‚  â””â”€ conditioning.py    # cond2_via_svd
â”œâ”€ s4_3/
â”‚  â”œâ”€ s4_3.py
â”‚  â””â”€ grad_theory.py     # descent-lemma demos (didactic)
â”œâ”€ s4_4/
â”‚  â”œâ”€ s4_4.py
â”‚  â””â”€ stable_reductions.py # logsumexp, log_softmax, softplus
â”œâ”€ s4_5/
â”‚  â”œâ”€ s4_5.py
â”‚  â””â”€ gradcheck.py       # central-difference gradient check
â”œâ”€ api.py
â””â”€ tests/
```

**Ch04 exports used later:** `log_softmax` (stable CE path) and `gradcheck` (backprop verification).

---

### ch05_ml_basics/

```
chapters/ch05_ml_basics/
â”œâ”€ s5_1/
â”‚  â”œâ”€ s5_1.py
â”‚  â””â”€ learning_setup.py  # task T, performance P, experience E
â”œâ”€ s5_2/
â”‚  â”œâ”€ s5_2.py
â”‚  â””â”€ capacity.py        # basic under/overfitting diagnostics
â”œâ”€ s5_3/
â”‚  â”œâ”€ s5_3.py
â”‚  â””â”€ splits_hparams.py  # train/val/test split
â”œâ”€ s5_9/
â”‚  â”œâ”€ s5_9.py
â”‚  â””â”€ sgd_basics.py      # batch_iterator, seeding
â”œâ”€ s5_10/
â”‚  â”œâ”€ s5_10.py
â”‚  â””â”€ system_design.py   # simple CSV logging helpers
â”œâ”€ api.py
â””â”€ tests/
```

**Ch05 exports used later:** seeding, splits, minibatches, accuracy, lightweight logging.

---

### ch06_feedforward/

```
chapters/ch06_feedforward/
â”œâ”€ s6_1/
â”‚  â”œâ”€ s6_1.py
â”‚  â””â”€ layers.py          # linear_forward/backward (cache X)
â”œâ”€ s6_2/
â”‚  â”œâ”€ s6_2.py
â”‚  â””â”€ activations.py     # relu_forward/backward (mask from Z)
â”œâ”€ s6_3/
â”‚  â”œâ”€ s6_3.py
â”‚  â””â”€ init.py            # glorot_uniform
â”œâ”€ s6_4/
â”‚  â”œâ”€ s6_4.py
â”‚  â””â”€ losses.py          # cross_entropy_from_logits (calls ch04.log_softmax)
â”œâ”€ s6_5/
â”‚  â”œâ”€ s6_5.py
â”‚  â””â”€ backprop_identities.py # ce_grad_wrt_logits = (softmax - onehot)/B
â”œâ”€ api.py
â””â”€ tests/
```

**Ch06 exports used later:** all building blocks of the MLP forward/backward and stable CE.

---

### ch07_regularization/

```
chapters/ch07_regularization/
â”œâ”€ s7_1/
â”‚  â”œâ”€ s7_1.py
â”‚  â””â”€ weight_decay.py    # l2_value (Î»/2 Î£||W||Â²), l2_grad (Î»W), exclude_biases
â”œâ”€ api.py
â””â”€ tests/
```

**Ch07 exports used later:** L2 value + gradients for weights (biases excluded).

---

### ch08_optimization/

```
chapters/ch08_optimization/
â”œâ”€ s8_1/
â”‚  â”œâ”€ s8_1.py
â”‚  â””â”€ sgd.py             # sgd_update (in-place)
â”œâ”€ s8_2/
â”‚  â”œâ”€ s8_2.py
â”‚  â””â”€ momentum.py        # momentum_update (in-place)
â”œâ”€ s8_3/
â”‚  â”œâ”€ s8_3.py
â”‚  â””â”€ schedules.py       # cosine_lr (optional)
â”œâ”€ api.py
â””â”€ tests/
```

**Ch08 exports used later:** parameter update rules, optional LR schedules.

---

## ğŸ§ª MNIST App (Composes Chapters Only)

```
app_mnist/
â”œâ”€ data_mnist.py         # uses ch05: splits, batches, seeding
â”œâ”€ model_mlp.py          # uses ch06: layers/activations/loss; ch07: L2; ch04: log_softmax (via ch06)
â”œâ”€ train.py              # uses ch08: sgd/momentum; ch05: logging, accuracy
â”œâ”€ metrics.py            # uses ch05: accuracy_from_logits
â””â”€ tests/
   â”œâ”€ test_softmax_ce.py # stability & shift invariance
   â”œâ”€ test_gradcheck.py  # tiny batch finite-diff check
   â””â”€ test_end_to_end_smoke.py # 1-epoch smoke: CEâ†“, accâ†‘
```

**Wiring (data/grad flow):**

```
batches â†’ forward â†’ CE (log-softmax) + L2 â†’ backward (dZ2 etc.) â†’ sgd/momentum â†’ val/test metrics
```

---

## ğŸ”— Dependency Graph (who imports whom)

```
ch02_linear_algebra  â”
ch03_probability     â”‚  (semantics/tests; optional at runtime)
                     â”œâ”€â”€â–¶ ch04_numerical â”€â”€â–¶ ch06_feedforward (loss uses ch04.log_softmax)
ch05_ml_basics  â”€â”€â”€â”€â”€â”˜                        â–²
                                              â”‚
ch07_regularization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (L2 value + grads)
ch08_optimization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ app_mnist/train.py

app_mnist/model_mlp.py  â—€â”€â”€ ch06 (forward/backward) + ch07 (L2)
app_mnist/data_mnist.py â—€â”€â”€ ch05 (splits/batching)
app_mnist/metrics.py    â—€â”€â”€ ch05 (accuracy)
```

---

## ğŸ§­ Build Order (keeps the repo runnable at all times)

1) **Ch04** stable reductions (`logsumexp`, `log_softmax`) + **gradcheck**  
2) **Ch06** layers, ReLU, Glorot; CE from logits; CE grad identity  
3) **Ch07** L2 value/grad (weights only)  
4) **Ch05** splits, batching, accuracy, seeding, basic logging  
5) **App**: model wrapper, training loop, metrics; smoke tests  
6) **Ch03** probability semantics & invariance tests for README plots  
7) **Ch02** linear algebra utilities (used for diagnostics & extensions)

---

## âœ… Testing

- Use `pytest` (or `python -m pytest`) to run everything in `chapters/**/tests` and `app_mnist/tests`.
- Core invariants:
  - Softmax rows sum to 1; CE is **shift-invariant** (add constant per row of logits â†’ unchanged).
  - Gradcheck (central difference) agrees with backprop within tolerance (float64 on tiny batch).
  - L2 penalizes **weights only**.

---

## ğŸ› ï¸ Conventions

- **NumPy-only** (`numpy` is the only array library; no autograd).
- Prefer **float32** for training; switch to **float64** only for gradcheck.
- Shapes explicit in docstrings (e.g., `X: (B,784)`).
- Deterministic runs via seeded RNG (chapter 5 API).
- Keep functions **small**; section `sX_Y.py` re-exports helpers so import sites stay clean.

---

## ğŸ§© Example Imports (no frameworks, only chapter APIs)

```python
# MNIST app uses only chapter APIs
from chapters.ch06_feedforward.api   import linear_forward, relu_forward, cross_entropy_from_logits, ce_grad_wrt_logits, glorot_uniform
from chapters.ch07_regularization.api import l2_value, l2_grad
from chapters.ch08_optimization.api  import sgd_update, momentum_update
from chapters.ch05_ml_basics.api     import train_val_test_split, batch_iterator, accuracy_from_logits, set_all_seeds
```

---

## ğŸ“„ License

MIT â€” free to use, fork, and remix (for learning and research).
