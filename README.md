# 🧠 Deep Learning From Scratch (2016 Mode)

A full manual rebuild of the Deep Learning Book (2016) by Ian Goodfellow, Yoshua Bengio, and Aaron Courville — line by line, concept by concept, from first principles.

No libraries.  
No frameworks.  
Only raw Python, matrix math, and full control.

---

## 🚀 Project Goal

To master deep learning by implementing every foundational concept from scratch — including:

- Linear Algebra
- Probability and Information Theory
- Calculus and Gradients
- Optimization (SGD, Momentum, etc.)
- Neural Network Layers
- Autograd and Backprop
- Training Loops and Loss Functions

> **By the end**: this repo becomes a working educational deep learning engine, built from zero.

---

## 🧱 Structure

```
deep-learning-from-scratch/
│
├── core/                  # All core logic (by chapter/topic)
│   ├── linear_algebra/    # Matrix ops, norms, dot product, eigens
│   ├── calculus/          # Derivatives, gradients
│   ├── probability/       # Distributions, entropy
│   ├── optimization/      # SGD, Adam, momentum
│   ├── nn/                # Layers, activations, forward pass
│   ├── autograd/          # Manual autograd engine (later)
│
├── demos/                 # Working examples using core code
├── tests/                 # Tests for each module
├── docs/                  # Notes and markdowns for theory
├── notebooks/             # Optional Jupyter notebooks
├── assets/                # Diagrams and visuals
└── playground/            # Experimental scratchpad
```


---

## 📚 Based On

- [Deep Learning (MIT Press)](https://www.deeplearningbook.org/)
- All math, code, and logic is faithful to the 2016 edition

---

## 🧠 Philosophy

This repo is not about speed or convenience.  
It’s about **understanding**, **control**, and **depth**.

Everything is built to learn:
- You will write dot products manually
- You will simulate gradients before using autograd
- You will debug matrix shapes and flows by hand

If you can build this, you’ll never fear PyTorch or TensorFlow again.

---

## ✅ Progress

| Chapter                     | Status     |
|-----------------------------|------------|
| Linear Algebra              | 🟢 Started |
| Probability & Information   | ⬜️ Pending |
| Calculus & Gradients        | ⬜️ Pending |
| Optimization                | ⬜️ Pending |
| Neural Networks             | ⬜️ Pending |
| Backprop / Autograd         | ⬜️ Pending |

---

## 📜 License

MIT License — use, fork, or remix freely.

---
